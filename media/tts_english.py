import asyncio
import edge_tts
import os

class TTSEngine:
    default_voice = "ne-NP-HemkalaNeural"

    @staticmethod
    async def generate_multivocal_audio(segments: list, output_path: str):
        """
        Generates audio for multiple segments with alternating voices and merges offsets.
        """
        all_offsets = []
        cumulative_duration = 0
        temp_audio_files = []
        
        voice_map = {
            "female": "ne-NP-HemkalaNeural",
            "male": "ne-NP-SagarNeural"
        }

        for i, seg in enumerate(segments):
            temp_path = f"storage/temp_seg_{i}.mp3"
            voice = voice_map.get(seg.get("gender"), "female")
            
            # For news items, prepend the headline if present
            text_to_speak = seg.get("text", "")
            if seg.get("type") == "news" and seg.get("headline"):
                text_to_speak = f"{seg['headline']}। {text_to_speak}"
            
            _, offsets = await TTSEngine.generate_audio(text_to_speak, temp_path, voice)
            
            # Adjust offsets based on cumulative duration
            for off in offsets:
                off["start"] += cumulative_duration
                all_offsets.append(off)
            
            # Get actual duration of this clip
            from moviepy.editor import AudioFileClip
            clip = AudioFileClip(temp_path)
            cumulative_duration += clip.duration
            temp_audio_files.append(temp_path)
            clip.close()

        # Concatenate audio files
        from moviepy.editor import concatenate_audioclips, AudioFileClip
        clips = [AudioFileClip(f) for f in temp_audio_files]
        final_audio = concatenate_audioclips(clips)
        final_audio.write_audiofile(output_path, fps=44100)
        
        # Cleanup
        for c in clips: c.close()
        for f in temp_audio_files: 
            try: os.remove(f)
            except: pass

        return output_path, all_offsets

    @staticmethod
    async def generate_audio(text: str, output_path: str, voice: str = None):
        """
        Generates high-quality Nepali narration and word-level timestamps.
        Returns: (audio_path, word_offsets)
        """
        # --- Normalization ---
        import re
        text = text.strip()

        # Expand abbreviations (Doctor, Engineer, etc.)
        abbreviations = {
            r'डा\.': 'डाक्टर',
            r'इ\.': 'इन्जिनियर',
            r'ई\.': 'इन्जिनियर',
            r'प्रा\.': 'प्राध्यापक',
            r'प\.': 'पण्डित',
            r'वि\.सं\.': 'विक्रम सम्बत',
            r'नं\.': 'नम्बर',
            r'कि\.मी\.': 'किलोमिटर',
            r'मि\.': 'मिटर'
        }
        for abbr, full in abbreviations.items():
            # Use negative lookbehind/lookahead to avoid matching inside other words if needed, 
            # but in Nepali, these abbreviations are very specific with the dot.
            text = re.sub(abbr, full, text)

        # Remove redundant whitespace and newlines
        text = re.sub(r'\s+', ' ', text)
        # Ensure punctuation has a space after it for better TTS breathing
        text = re.sub(r'([।.,!?])(?=[^\s])', r'\1 ', text)
        
        if not text:
            print("WARNING: Empty text provided to TTS.")
            return output_path, []

        if not voice:
            # HemkalaNeural is the standard high-quality Nepali voice
            voice = "ne-NP-HemkalaNeural"
            
        print(f"Generating TTS for {len(text)} characters using voice: {voice}")
        communicate = edge_tts.Communicate(text, voice, rate="+20%")
        word_offsets = []
        
        try:
            audio_data = bytearray()
            async for chunk in communicate.stream():
                ctype = chunk.get("type") or chunk.get("Type") or "unknown"
                
                if ctype == "audio":
                    audio_data.extend(chunk["data"])
                elif ctype == "WordBoundary":
                    word_offsets.append({
                        "word": chunk.get("text") or chunk.get("Text"),
                        "start": (chunk.get("offset") or chunk.get("Offset")) / 10**7,
                        "duration": (chunk.get("duration") or chunk.get("Duration")) / 10**7
                    })

            if audio_data:
                with open(output_path, "wb") as f:
                    f.write(audio_data)
            else:
                print("CRITICAL: No audio data generated by TTS.")

        except Exception as e:
            print(f"Error during TTS streaming: {e}")
        
        # --- FALLBACK: Simulated Word Offsets ---
        if not word_offsets and len(text.strip()) > 0:
            print(f"NOTICE: edge-tts did not provide WordBoundary. Running Simulated Sync for: '{text[:50]}...'")
            from moviepy.editor import AudioFileClip
            try:
                temp_audio = AudioFileClip(output_path)
                total_dur = temp_audio.duration
                temp_audio.close()
            except Exception as e:
                print(f"Warning: Could not read audio duration, estimating... ({e})")
                total_dur = len(text.split()) * 0.4 # Rough estimate
            
            try:
                words = text.split()
                start_time = 0
                total_chars = sum(len(x) for x in words)
                for w in words:
                    # Logic: use word length proportionality
                    w_dur = (len(w) / total_chars) * total_dur
                    word_offsets.append({
                        "word": w,
                        "start": start_time,
                        "duration": w_dur
                    })
                    start_time += w_dur
            except Exception as e:
                print(f"Final simulated sync attempt failed: {e}")

        if not word_offsets:
            print(f"CRITICAL: Failed to generate word offsets for synchronization.")
        else:
            # print(f"SUCCESS: {len(word_offsets)} word milestones ready for animation.")
            pass
        
        return output_path, word_offsets

if __name__ == "__main__":
    # Test generation
    segments = [
        {"type": "intro", "text": "नमस्कार, नेपाल नाउमा हजुरलाइ स्वागत छ", "gender": "female"},
        {"type": "news", "headline": "मौसम अपडेट", "text": "आज काठमाडौंमा भारी वर्षाको सम्भावना छ।", "gender": "male"}
    ]
    output = "test_multivocal.mp3"
    asyncio.run(TTSEngine.generate_multivocal_audio(segments, output))
